{
 "metadata": {
  "name": "comment_labeling"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "NPS_Chat corpus has been labeled with dialogue acts. There are 10,567 tagged chat conversations and tags are:\n",
      "\n",
      "Emotion\n",
      "ynQuestion\n",
      "yAnswer\n",
      "Continuer\n",
      "whQuestion\n",
      "System\n",
      "Accept\n",
      "Clarify\n",
      "Emphasis\n",
      "nAnswer\n",
      "Greet\n",
      "Statement\n",
      "Reject\n",
      "Bye\n",
      "Other\n",
      "\n",
      "\n",
      "The general idea is to build and test a classifier for chats and then apply this classifier to our github diliberations comment corpus to determine if there is a relationship between comment type and participation in future dicussions.\n",
      "\n",
      "We have decided to try this instead of manually labeling \n",
      "\n",
      "1. Create test, training, and a held out set for the corpus. \n",
      "2. Build naive bayes classifier from nps_chat corpus\n",
      "3. Classify conversations from the github comments\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Ideas for features:\n",
      "\n",
      "from Charles paper (http://www.collide.info/sites/default/files/Christopher_Charles_Masterarbeit.pdf)\n",
      "- the first 2 words as a bigram\n",
      "- the last token\n",
      "- whether there is a question mark at the end\n",
      "- whether there is an exclamation mark at the end\n",
      "- whether the post's first few tokens contain a token from a list deemed positive (\"yes\",\"alright\",\"sure\",\"cool\")\n",
      "- whether the post's first few tokens contain a token from a list deemed negative (\"no\", \"not\", \"aint\", \"dont\")\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk import corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "There are 10,567 tagged chat conversations in the NPS_chat corpus.\n",
      "The tags include:\n",
      "emotion, ynQuestion, yAnswer, Continuer, whQuestion, System, Accept, Clarify, Emphasis, nAnswer, Greet, Statement, reject, bye, other"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(nltk.corpus.nps_chat.xml_posts())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "10567"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts = nltk.corpus.nps_chat.xml_posts()[:10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Look at examples of the tags.\n",
      "#Found that most of the sentences are short and mainly 'chat-speark'\n",
      "\n",
      "for i in posts[0:100]:\n",
      "    if i.get(\"class\") == \"Emotion\":\n",
      "        print i.text, i.get(\"class\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ":P Emotion\n",
        ":) Emotion\n",
        "boo. Emotion\n",
        "lol 10-19-20sUser115 Emotion\n",
        "boo. Emotion\n",
        "boo. Emotion\n",
        "ewwwww lol Emotion\n",
        "heeeey! Emotion\n",
        ":) Emotion\n",
        "haha Emotion\n",
        "opps Emotion\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Simple feature extractor for unigrams\n",
      "def dialogue_act_features(post):\n",
      "    features = {}\n",
      "    for word in nltk.word_tokenize(post):\n",
      "        features['contains(%s)' % word.lower()] = True\n",
      "        \n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Testing out the classifier. It is .67 accurate\n",
      "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
      "                        for post in posts]\n",
      "size = int(len(featuresets) * 0.1)\n",
      "train_set, test_set = featuresets[size:], featuresets[:size]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print nltk.classify.accuracy(classifier, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.67\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Importing comments file and concatenating comments that go to multiple lines\n",
      "import re\n",
      "\n",
      "f = open('Dropbox/Berkeley/nlp/finalproject/nlp_github_deliberations/issues_conversation_details_all.tsv', 'r')\n",
      "comments = []\n",
      "last_good_line = 0\n",
      "counter = 0\n",
      "for line in f.readlines():\n",
      "    if re.match('^\\d+\\t\\d+\\th', line):\n",
      "        last_good_line = counter\n",
      "        comments.append(line.split(\"\\t\"))\n",
      "        counter += 1\n",
      "        \n",
      "    if not re.match('^\\d+\\t\\d+\\th', line):\n",
      "        comments.append(line)\n",
      "        comments[last_good_line][4] += line\n",
      "        counter += 1\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_comments = []\n",
      "for i in comments:\n",
      "    if isinstance(i, list):\n",
      "        i[4] = i[4].replace('\\n',' ')\n",
      "        i[4] = i[4].replace('\\t','    ')\n",
      "        clean_comments.append(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in clean_comments:\n",
      "    if i[1] == '28045983':\n",
      "        print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['3219', '28045983', 'https://api.github.com/repos/bitcoin/bitcoin/issues/comments/28045983', 'laanwj', 'I just upgraded my laptop to Ubuntu 13.10 (Salamander Sauce). This indeed installed boost 1.53:      libboost-chrono1.53-dev:amd64            install     libboost-date-time1.53-dev:amd64        install     libboost-dev:amd64                install     libboost-filesystem-dev:amd64            install     libboost-filesystem1.53-dev:amd64        install     libboost-iostreams-dev                install     libboost-iostreams1.53-dev:amd64        install     libboost-program-options-dev:amd64        install     libboost-program-options1.53-dev:amd64        install     libboost-regex1.53-dev:amd64            install     libboost-serialization1.53-dev:amd64        install     libboost-system-dev:amd64            install     libboost-system1.53-dev:amd64            install     libboost-test-dev:amd64                install     libboost-test1.53-dev:amd64            install     libboost-thread-dev:amd64            install     libboost-thread1.53-dev:amd64            install     libboost1.53-dev:amd64                install  No problems encountered configuring bitcoin:      checking for boostlib >= 1.20.0... yes     checking whether the Boost::System library is available... yes     checking for exit in -lboost_system... yes     checking whether the Boost::Filesystem library is available... yes     checking for exit in -lboost_filesystem... yes     checking whether the Boost::Program_Options library is available... yes     checking for exit in -lboost_program_options... yes     checking whether the Boost::Thread library is available... yes     checking for exit in -lboost_thread... yes     checking whether the Boost::Chrono library is available... yes     checking whether the Boost::Unit_Test_Framework library is available... yes  Did you install all the necessary libboost-XXX-dev dependency packages?  ']\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Classify using the simple unigram classifier\n",
      "f = open('classifier_output.tsv','w')\n",
      "for comment in clean_comments:\n",
      "    f.write(comment[0] + \"\\t \"+ comment[1] + \"\\t\" + comment[4][0:300].replace('\\n',' ') + '\\t' + classifier.classify(dialogue_act_features(comment[4])) + '\\n')\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate file for our own labeling\n",
      "f = open('label2.tsv','w')\n",
      "for comment in clean_comments:\n",
      "    if comment[3] != \"BitcoinPullTester\":\n",
      "            f.write(comment[0] + \"\\t \"+ comment[1] + \"\\t\" + comment[4][0:300].replace('\\n',' ') + '\\n')\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    }
   ],
   "metadata": {}
  }
 ]
}